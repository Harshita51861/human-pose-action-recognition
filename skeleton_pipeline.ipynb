import cv2
import numpy as np
from ultralytics import YOLO

# =============== CONFIG =================
VIDEO_PATH = "/kaggle/input/multiple/multiple.mp4"  # <-- change this
MAX_FRAMES   = None    # or set like 300 to limit
MAX_PEOPLE   = 5       # max persons to store per frame
CONF_THRESH  = 0.25    # detection confidence threshold
KP_CONF_MIN  = 0.30    # min avg keypoint confidence to keep that person
# ========================================

# Load YOLOv8 pose model (auto-downloads yolov8n-pose.pt)
model = YOLO("yolov8n-pose.pt")

cap = cv2.VideoCapture(VIDEO_PATH)
if not cap.isOpened():
    raise ValueError("Cannot open video. Check VIDEO_PATH.")

all_frames = []
frame_idx = 0

while True:
    ret, frame = cap.read()
    if not ret:
        break

    frame_idx += 1
    if MAX_FRAMES is not None and frame_idx > MAX_FRAMES:
        break

    # Run YOLO pose on this frame
    results = model(frame, conf=CONF_THRESH, verbose=False)

    # results[0] is first image result
    res = results[0]

    # keypoints: (num_people, 17, 3) -> x,y,conf
    if res.keypoints is None:
        # no people
        all_frames.append(np.zeros((0, 17, 3), dtype=np.float32))
        continue

    kps = res.keypoints.data.cpu().numpy().astype(np.float32)  # (P, 17, 3)

    # Filter persons based on avg keypoint confidence
    filtered_persons = []
    for p in range(kps.shape[0]):
        kp_person = kps[p]   # (17, 3)
        avg_conf = kp_person[:, 2].mean()
        if avg_conf >= KP_CONF_MIN:
            filtered_persons.append(kp_person)

    if len(filtered_persons) == 0:
        all_frames.append(np.zeros((0, 17, 3), dtype=np.float32))
    else:
        all_frames.append(np.stack(filtered_persons, axis=0))   # (P_f, 17, 3)

cap.release()

print(f"Total frames processed: {len(all_frames)}")

import cv2
import time
import matplotlib.pyplot as plt
from IPython.display import display, clear_output
import ipywidgets as widgets
from ultralytics import YOLO

# ================== CONFIG ==================
VIDEO_PATH = "/kaggle/input/multiple/multiple.mp4"  # <-- CHANGE THIS
CONF_THRESH = 0.25            # detection confidence
DISPLAY_SIZE = (7, 5)         # matplotlib figure size
MAX_FRAMES = None             # or set e.g. 400 to limit
# ===========================================

# 1) LOAD MODEL
model = YOLO("yolov8n-pose.pt")   # lightweight multi-person pose

# 2) PRECOMPUTE ANNOTATED FRAMES (with skeletons)
cap = cv2.VideoCapture(VIDEO_PATH)
if not cap.isOpened():
    raise ValueError("Cannot open video. Check VIDEO_PATH.")

annotated_frames = []
frame_idx = 0

print("Processing video and drawing skeletons...")

while True:
    ret, frame = cap.read()
    if not ret:
        break

    frame_idx += 1
    if MAX_FRAMES is not None and frame_idx > MAX_FRAMES:
        break

    results = model(frame, conf=CONF_THRESH, verbose=False)[0]
    annotated = results.plot()  # draw all people (bones + joints)
    annotated = cv2.cvtColor(annotated, cv2.COLOR_BGR2RGB)
    annotated_frames.append(annotated)

cap.release()

num_frames = len(annotated_frames)
print(f"Done. Total frames with skeletons: {num_frames}")

if num_frames == 0:
    raise ValueError("No frames were processed. Check your video.")

# 3) CREATE PLAYER STATE + UI
current_idx = 0
playing = False

out_frame = widgets.Output()

btn_play   = widgets.Button(description="▶ Play", button_style="success")
btn_pause  = widgets.Button(description="⏸ Pause", button_style="warning")
btn_prev   = widgets.Button(description="⏮ Prev")
btn_next   = widgets.Button(description="⏭ Next")
btn_restart= widgets.Button(description="⏹ Restart", button_style="danger")

slider = widgets.IntSlider(
    value=0,
    min=0,
    max=num_frames - 1,
    step=1,
    description='Frame:',
    continuous_update=False,
    layout=widgets.Layout(width='80%')
)

controls = widgets.HBox([btn_play, btn_pause, btn_prev, btn_next, btn_restart])
ui = widgets.VBox([controls, slider, out_frame])

# 4) DISPLAY FRAME FUNCTION
def show_frame(idx):
    idx = max(0, min(num_frames - 1, idx))
    frame = annotated_frames[idx]

    with out_frame:
        clear_output(wait=True)
        plt.figure(figsize=DISPLAY_SIZE)
        plt.imshow(frame)
        plt.axis("off")
        plt.title(f"Frame {idx+1}/{num_frames}")
        plt.show()

# 5) BUTTON CALLBACKS
def on_play_clicked(b):
    global playing, current_idx
    playing = True
    while playing and current_idx < num_frames:
        slider.value = current_idx
        show_frame(current_idx)
        current_idx += 1
        time.sleep(0.04)  # playback speed (~25 fps)

def on_pause_clicked(b):
    global playing
    playing = False

def on_prev_clicked(b):
    global current_idx
    current_idx = max(0, current_idx - 1)
    slider.value = current_idx
    show_frame(current_idx)

def on_next_clicked(b):
    global current_idx
    current_idx = min(num_frames - 1, current_idx + 1)
    slider.value = current_idx
    show_frame(current_idx)

def on_restart_clicked(b):
    global current_idx, playing
    playing = False
    current_idx = 0
    slider.value = current_idx
    show_frame(current_idx)

def on_slider_change(change):
    global current_idx, playing
    if change["name"] == "value":
        playing = False  # stop autoplay when user scrubs
        current_idx = change["new"]
        show_frame(current_idx)

btn_play.on_click(on_play_clicked)
btn_pause.on_click(on_pause_clicked)
btn_prev.on_click(on_prev_clicked)
btn_next.on_click(on_next_clicked)
btn_restart.on_click(on_restart_clicked)
slider.observe(on_slider_change)

# 6) SHOW UI + FIRST FRAME
display(ui)
show_frame(0)

import numpy as np

MAX_PEOPLE = 5   # keep up to 5 people per frame
KP_CONF_MIN = 0.30   # minimum average kp confidence to keep a person

multi_skeleton = []   # will become [T, P, 17, 3]

print("Extracting keypoint matrices...")

for frame in annotated_frames:  
    # Use YOLO again only for keypoints (lightweight)
    results = model(frame, conf=CONF_THRESH, verbose=False)[0]

    if results.keypoints is None:
        multi_skeleton.append(np.zeros((0, 17, 3), dtype=np.float32))
        continue

    kps = results.keypoints.data.cpu().numpy().astype(np.float32)  # (people, 17, 3)

    filtered = []
    for person in kps:
        avg_conf = person[:, 2].mean()
        if avg_conf >= KP_CONF_MIN:
            filtered.append(person)

    if len(filtered) == 0:
        multi_skeleton.append(np.zeros((0, 17, 3), dtype=np.float32))
    else:
        multi_skeleton.append(np.stack(filtered))

T = len(multi_skeleton)
J = 17  # joints
C = 3   # (x, y, conf)

multi_skeleton_tensor = np.zeros((T, MAX_PEOPLE, J, C), dtype=np.float32)

for t, persons in enumerate(multi_skeleton):
    if persons.shape[0] == 0:
        continue
    
    n = min(persons.shape[0], MAX_PEOPLE)
    multi_skeleton_tensor[t, :n, :, :] = persons[:n]

np.save("/kaggle/working/skeleton_data.npy", multi_skeleton_tensor)
print("Saved multi-person skeleton matrix at /kaggle/working/skeleton_data.npy")
print("Matrix shape:", multi_skeleton_tensor.shape)

frame0 = matrix[0]
print("First frame shape:", frame0.shape)
print(frame0)

import numpy as np

# ================== CONFIG ==================
SKELETON_PATH = "/kaggle/working/skeleton_data.npy"   # from previous step
CLIP_LEN      = 60    # frames per clip
STRIDE        = 30    # step between clip starts
ACTION_LABEL  = 0     # integer label for this video's action
# ============================================

# 1) Load skeleton matrix
skeleton = np.load(SKELETON_PATH)
print("Skeleton shape:", skeleton.shape)
# expected: (T, MAX_PEOPLE, J, C)

T, MAX_PEOPLE, J, C = skeleton.shape

# 2) Create clips
clips = []
labels = []

if T <= CLIP_LEN:
    # Not enough frames -> pad to CLIP_LEN
    pad_len = CLIP_LEN - T
    pad = np.zeros((pad_len, MAX_PEOPLE, J, C), dtype=skeleton.dtype)
    clip = np.concatenate([skeleton, pad], axis=0)  # (CLIP_LEN, ...)
    clips.append(clip)
    labels.append(ACTION_LABEL)
else:
    # Sliding window over time
    start = 0
    while start + CLIP_LEN <= T:
        clip = skeleton[start:start+CLIP_LEN]   # (CLIP_LEN, MAX_PEOPLE, J, C)
        clips.append(clip)
        labels.append(ACTION_LABEL)
        start += STRIDE

    # Optionally: handle leftover at the end (pad last partial clip)
    if start < T:
        last_segment = skeleton[start:]
        pad_len = CLIP_LEN - last_segment.shape[0]
        if pad_len > 0:
            pad = np.zeros((pad_len, MAX_PEOPLE, J, C), dtype=skeleton.dtype)
            last_clip = np.concatenate([last_segment, pad], axis=0)
            clips.append(last_clip)
            labels.append(ACTION_LABEL)

# 3) Convert to arrays
X_clips = np.stack(clips, axis=0)      # (N_clips, CLIP_LEN, MAX_PEOPLE, J, C)
y_clips = np.array(labels, dtype=np.int64)  # (N_clips,)

print("X_clips shape:", X_clips.shape)
print("y_clips shape:", y_clips.shape)
print("Unique labels:", np.unique(y_clips))

# 4) Save for later training
np.save("/kaggle/working/X_clips.npy", X_clips)
np.save("/kaggle/working/y_clips.npy", y_clips)

print("Saved:")
print("  /kaggle/working/X_clips.npy")
print("  /kaggle/working/y_clips.npy")

import numpy as np

NUM_JOINTS = 17

# COCO-style edges for YOLOv8 Pose (17 keypoints)
EDGES = [
    (5, 6),
    (5, 7), (7, 9),
    (6, 8), (8, 10),
    (11, 12),
    (5, 11), (6, 12),
    (11, 13), (13, 15),
    (12, 14), (14, 16),
    (0, 5), (0, 6),
    (0, 1), (1, 3),
    (0, 2), (2, 4),
]

A = np.zeros((NUM_JOINTS, NUM_JOINTS), dtype=np.float32)
for i, j in EDGES:
    A[i, j] = 1.0
    A[j, i] = 1.0

for i in range(NUM_JOINTS):
    A[i, i] = 1.0  # self-connections

# Normalize adjacency: D^{-1/2} A D^{-1/2}
D = A.sum(axis=1)
D_inv_sqrt = np.power(D, -0.5, where=D > 0)
D_inv_sqrt[D <= 0] = 0.0
D_mat = np.diag(D_inv_sqrt)
A_norm = D_mat @ A @ D_mat   # (17,17)

np.save("/kaggle/working/adjacency_17j.npy", A_norm)
print("Adjacency shape:", A_norm.shape)
